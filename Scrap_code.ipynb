{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting selenium\n",
      "  Downloading selenium-4.19.0-py3-none-any.whl (10.5 MB)\n",
      "Collecting certifi>=2021.10.8\n",
      "  Downloading certifi-2024.2.2-py3-none-any.whl (163 kB)\n",
      "Collecting trio~=0.17\n",
      "  Downloading trio-0.25.0-py3-none-any.whl (467 kB)\n",
      "Collecting urllib3[socks]<3,>=1.26\n",
      "  Downloading urllib3-2.2.1-py3-none-any.whl (121 kB)\n",
      "Collecting trio-websocket~=0.9\n",
      "  Downloading trio_websocket-0.11.1-py3-none-any.whl (17 kB)\n",
      "Collecting typing_extensions>=4.9.0\n",
      "  Downloading typing_extensions-4.11.0-py3-none-any.whl (34 kB)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\dgarg\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.2.2)\n",
      "Collecting sniffio>=1.3.0\n",
      "  Downloading sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Collecting exceptiongroup; python_version < \"3.11\"\n",
      "  Downloading exceptiongroup-1.2.1-py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: cffi>=1.14; os_name == \"nt\" and implementation_name != \"pypy\" in c:\\users\\dgarg\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.14.0)\n",
      "Requirement already satisfied: idna in c:\\users\\dgarg\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.10)\n",
      "Collecting attrs>=23.2.0\n",
      "  Downloading attrs-23.2.0-py3-none-any.whl (60 kB)\n",
      "Collecting outcome\n",
      "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6; extra == \"socks\" in c:\\users\\dgarg\\anaconda3\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Collecting wsproto>=0.14\n",
      "  Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: pycparser in c:\\users\\dgarg\\anaconda3\\lib\\site-packages (from cffi>=1.14; os_name == \"nt\" and implementation_name != \"pypy\"->trio~=0.17->selenium) (2.20)\n",
      "Collecting h11<1,>=0.9.0\n",
      "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Installing collected packages: certifi, sniffio, exceptiongroup, attrs, outcome, trio, urllib3, h11, wsproto, trio-websocket, typing-extensions, selenium\n",
      "  Attempting uninstall: certifi\n",
      "    Found existing installation: certifi 2020.6.20\n",
      "    Uninstalling certifi-2020.6.20:\n",
      "      Successfully uninstalled certifi-2020.6.20\n",
      "  Attempting uninstall: attrs\n",
      "    Found existing installation: attrs 19.3.0\n",
      "Note: you may need to restart the kernel to use updated packages.    Uninstalling attrs-19.3.0:\n",
      "      Successfully uninstalled attrs-19.3.0\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.25.9\n",
      "    Uninstalling urllib3-1.25.9:\n",
      "      Successfully uninstalled urllib3-1.25.9\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing-extensions 3.7.4.2\n",
      "    Uninstalling typing-extensions-3.7.4.2:\n",
      "      Successfully uninstalled typing-extensions-3.7.4.2\n",
      "Successfully installed attrs-23.2.0 certifi-2024.2.2 exceptiongroup-1.2.1 h11-0.14.0 outcome-1.3.0.post0 selenium-4.19.0 sniffio-1.3.1 trio-0.25.0 trio-websocket-0.11.1 typing-extensions-4.11.0 urllib3-2.2.1 wsproto-1.2.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: requests 2.24.0 has requirement urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1, but you'll have urllib3 2.2.1 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'WebDriver' object has no attribute 'find_element_by_xpath'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-0262790880ac>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;31m# Scrape data from the URL\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscrape_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[1;31m# Print the number of entries scraped\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-0262790880ac>\u001b[0m in \u001b[0;36mscrape_data\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m         \u001b[1;31m# Find the \"Next\" button and click on it using XPath\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 48\u001b[1;33m         \u001b[0mnext_button\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_element_by_xpath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'//a[@class=\"js\"]'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     49\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnext_button\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m             \u001b[0mnext_button\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclick\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'WebDriver' object has no attribute 'find_element_by_xpath'"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_data(url):\n",
    "    # Initialize an empty list to store the scraped data\n",
    "    scraped_data = []\n",
    "\n",
    "    # Initialize the web driver\n",
    "    driver = webdriver.Chrome()  # You'll need to download chromedriver: https://sites.google.com/a/chromium.org/chromedriver/downloads\n",
    "\n",
    "    # Open the URL\n",
    "    driver.get(url)\n",
    "\n",
    "    while True:\n",
    "        # Parse the HTML content\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "        # Find the table containing the data\n",
    "        table = soup.find('table', class_='list_table')\n",
    "\n",
    "        # Check if the table exists\n",
    "        if table:\n",
    "            # Find all rows in the table\n",
    "            rows = table.find_all('tr')\n",
    "\n",
    "            # Loop through each row (skipping the header row)\n",
    "            for row in rows[1:]:\n",
    "                # Find all columns in the row\n",
    "                cols = row.find_all('td')\n",
    "\n",
    "                # Extract relevant data from each column\n",
    "                tender_id = cols[0].text.strip()\n",
    "                tender_description = cols[1].text.strip()\n",
    "                tender_organization = cols[2].text.strip()\n",
    "                tender_date = cols[3].text.strip()\n",
    "                other_column = cols[4].text.strip()  # Assuming there's one more column\n",
    "\n",
    "                # Append the data to the list\n",
    "                scraped_data.append({\n",
    "                    'Tender ID': tender_id,\n",
    "                    'Description': tender_description,\n",
    "                    'Organization': tender_organization,\n",
    "                    'Date': tender_date,\n",
    "                    'Other Column': other_column\n",
    "                })\n",
    "\n",
    "        # Find the \"Next\" button and click on it using XPath\n",
    "        next_button = driver.find_element_by_xpath('//a[@class=\"js\"]')\n",
    "        if next_button:\n",
    "            next_button.click()\n",
    "        else:\n",
    "            break  # Break the loop if there are no more pages\n",
    "\n",
    "    # Close the web driver\n",
    "    driver.quit()\n",
    "\n",
    "    return scraped_data\n",
    "\n",
    "# URL of the website\n",
    "url = \"https://eprocure.gov.in/cppp/resultoftendersnew/cpppdata/byYzJWc1pXTjBBMTNoMUExM2gxQTEzaDFBMTNoMU1qQXlNdz09QTEzaDFVSFZpYkdsemFHVms=\"\n",
    "\n",
    "# Scrape data from the URL\n",
    "data = scrape_data(url)\n",
    "\n",
    "# Print the number of entries scraped\n",
    "print(\"Total Entries Scraped:\", len(data))\n",
    "\n",
    "# Print the first few scraped entries\n",
    "for entry in data[:5]:\n",
    "    print(entry)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'WebDriver' object has no attribute 'find_element_by_class_name'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-b3b2a411d902>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;31m# Scrape data from the URL\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscrape_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[1;31m# Print the number of entries scraped\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-b3b2a411d902>\u001b[0m in \u001b[0;36mscrape_data\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m         \u001b[1;31m# Find the \"Next\" button and click on it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 48\u001b[1;33m         \u001b[0mnext_button\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_element_by_class_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'js'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     49\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnext_button\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m             \u001b[0mnext_button\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclick\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'WebDriver' object has no attribute 'find_element_by_class_name'"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_data(url):\n",
    "    # Initialize an empty list to store the scraped data\n",
    "    scraped_data = []\n",
    "\n",
    "    # Initialize the web driver\n",
    "    driver = webdriver.Chrome()  # You'll need to download chromedriver: https://sites.google.com/a/chromium.org/chromedriver/downloads\n",
    "\n",
    "    # Open the URL\n",
    "    driver.get(url)\n",
    "\n",
    "    while True:\n",
    "        # Parse the HTML content\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "        # Find the table containing the data\n",
    "        table = soup.find('table', class_='list_table')\n",
    "\n",
    "        # Check if the table exists\n",
    "        if table:\n",
    "            # Find all rows in the table\n",
    "            rows = table.find_all('tr')\n",
    "\n",
    "            # Loop through each row (skipping the header row)\n",
    "            for row in rows[1:]:\n",
    "                # Find all columns in the row\n",
    "                cols = row.find_all('td')\n",
    "\n",
    "                # Extract relevant data from each column\n",
    "                tender_id = cols[0].text.strip()\n",
    "                tender_description = cols[1].text.strip()\n",
    "                tender_organization = cols[2].text.strip()\n",
    "                tender_date = cols[3].text.strip()\n",
    "                other_column = cols[4].text.strip()  # Assuming there's one more column\n",
    "\n",
    "                # Append the data to the list\n",
    "                scraped_data.append({\n",
    "                    'Tender ID': tender_id,\n",
    "                    'Description': tender_description,\n",
    "                    'Organization': tender_organization,\n",
    "                    'Date': tender_date,\n",
    "                    'Other Column': other_column\n",
    "                })\n",
    "\n",
    "        # Find the \"Next\" button and click on it\n",
    "        next_button = driver.find_element_by_class_name('js')\n",
    "        if next_button:\n",
    "            next_button.click()\n",
    "        else:\n",
    "            break  # Break the loop if there are no more pages\n",
    "\n",
    "    # Close the web driver\n",
    "    driver.quit()\n",
    "\n",
    "    return scraped_data\n",
    "\n",
    "# URL of the website\n",
    "url = \"https://eprocure.gov.in/cppp/resultoftendersnew/cpppdata/byYzJWc1pXTjBBMTNoMUExM2gxQTEzaDFBMTNoMU1qQXlNdz09QTEzaDFVSFZpYkdsemFHVms=\"\n",
    "\n",
    "# Scrape data from the URL\n",
    "data = scrape_data(url)\n",
    "\n",
    "# Print the number of entries scraped\n",
    "print(\"Total Entries Scraped:\", len(data))\n",
    "\n",
    "# Print the first few scraped entries\n",
    "for entry in data[:5]:\n",
    "    print(entry)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'WebDriver' object has no attribute 'find_elements_by_xpath'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-bf578832d5db>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;31m# Scrape data from the URL\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscrape_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[1;31m# Print the number of entries scraped\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-bf578832d5db>\u001b[0m in \u001b[0;36mscrape_data\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m         \u001b[1;31m# Find all \"Next\" buttons and click on the first one using XPath\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 48\u001b[1;33m         \u001b[0mnext_buttons\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_elements_by_xpath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'//a[@class=\"js\"]'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     49\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnext_buttons\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m             \u001b[0mnext_buttons\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclick\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'WebDriver' object has no attribute 'find_elements_by_xpath'"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_data(url):\n",
    "    # Initialize an empty list to store the scraped data\n",
    "    scraped_data = []\n",
    "\n",
    "    # Initialize the web driver\n",
    "    driver = webdriver.Chrome()  # You'll need to download chromedriver: https://sites.google.com/a/chromium.org/chromedriver/downloads\n",
    "\n",
    "    # Open the URL\n",
    "    driver.get(url)\n",
    "\n",
    "    while True:\n",
    "        # Parse the HTML content\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "        # Find the table containing the data\n",
    "        table = soup.find('table', class_='list_table')\n",
    "\n",
    "        # Check if the table exists\n",
    "        if table:\n",
    "            # Find all rows in the table\n",
    "            rows = table.find_all('tr')\n",
    "\n",
    "            # Loop through each row (skipping the header row)\n",
    "            for row in rows[1:]:\n",
    "                # Find all columns in the row\n",
    "                cols = row.find_all('td')\n",
    "\n",
    "                # Extract relevant data from each column\n",
    "                tender_id = cols[0].text.strip()\n",
    "                tender_description = cols[1].text.strip()\n",
    "                tender_organization = cols[2].text.strip()\n",
    "                tender_date = cols[3].text.strip()\n",
    "                other_column = cols[4].text.strip()  # Assuming there's one more column\n",
    "\n",
    "                # Append the data to the list\n",
    "                scraped_data.append({\n",
    "                    'Tender ID': tender_id,\n",
    "                    'Description': tender_description,\n",
    "                    'Organization': tender_organization,\n",
    "                    'Date': tender_date,\n",
    "                    'Other Column': other_column\n",
    "                })\n",
    "\n",
    "        # Find all \"Next\" buttons and click on the first one using XPath\n",
    "        next_buttons = driver.find_elements_by_xpath('//a[@class=\"js\"]')\n",
    "        if next_buttons:\n",
    "            next_buttons[0].click()\n",
    "        else:\n",
    "            break  # Break the loop if there are no more pages\n",
    "\n",
    "    # Close the web driver\n",
    "    driver.quit()\n",
    "\n",
    "    return scraped_data\n",
    "\n",
    "# URL of the website\n",
    "url = \"https://eprocure.gov.in/cppp/resultoftendersnew/cpppdata/byYzJWc1pXTjBBMTNoMUExM2gxQTEzaDFBMTNoMU1qQXlNdz09QTEzaDFVSFZpYkdsemFHVms=\"\n",
    "\n",
    "# Scrape data from the URL\n",
    "data = scrape_data(url)\n",
    "\n",
    "# Print the number of entries scraped\n",
    "print(\"Total Entries Scraped:\", len(data))\n",
    "\n",
    "# Print the first few scraped entries\n",
    "for entry in data[:5]:\n",
    "    print(entry)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Entries Scraped: 10\n",
      "{'Tender ID': '1.', 'Description': '26-Feb-2024 12:00 AM', 'Organization': '31-Dec-2023 06:55 PM', 'Date': 'Cleaning, sweeping along with minor repairs as and when required at Regional Store premises under Talcher Area for a period of one year./ MCL/GM(TA)/SO(C)/ e-Tender/23-24/50 dt 30.12.23 / 2023_MCL_297374_1', 'Other Column': 'Mahanadi Coalfields Limited||TALCHER AREA||TALCH_CIVIL||CIVIL'}\n",
      "{'Tender ID': '2.', 'Description': '25-Feb-2024 12:00 AM', 'Organization': '31-Dec-2023 06:55 PM', 'Date': 'Providing and fixing of stainless steel gate and repairing of cow catcher at Mines Rescue Station Colony under Talcher Area./ MCL/GM(TA)/SO(C)/ e-Tender/23-24/49 dt 30.12.23 / 2023_MCL_297372_1', 'Other Column': 'Mahanadi Coalfields Limited||TALCHER AREA||TALCH_CIVIL||CIVIL'}\n",
      "{'Tender ID': '3.', 'Description': '29-Feb-2024 05:00 PM', 'Organization': '31-Dec-2023 05:00 PM', 'Date': '1)\\nBAR CT RATIO:800-400/1-1A, CORE-1: 10VA,5P10 ,CORE-II: 10VA,0.5,AS PER ANNEX.-A IT-02. Qty: 18 NO/ E4933097/4935222 / 4935222', 'Other Column': 'BHEL BHOPAL'}\n",
      "{'Tender ID': '4.', 'Description': '31-Mar-2024 05:00 PM', 'Organization': '31-Dec-2023 05:00 PM', 'Date': '1)22W LED TUBE LIGHT FOR RETROFITTING AS PER SPEC BP7410399. Qty: 500 NO/ E3833073/3830158 / 3830158', 'Other Column': 'BHEL BHOPAL'}\n",
      "{'Tender ID': '5.', 'Description': '08-Feb-2024 05:00 PM', 'Organization': '31-Dec-2023 05:00 PM', 'Date': '1)ALUMINIMUM SHIELDING TUBE L1=470, L=1170 AS PER VAR00 OF DRG. NO. 34528400012 REV01.\\n Qty: 1 NO/ E6933030/6935116 / 6935116', 'Other Column': 'BHEL BHOPAL'}\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_data(url):\n",
    "    # Initialize an empty list to store the scraped data\n",
    "    scraped_data = []\n",
    "\n",
    "    # Initialize the web driver\n",
    "    driver = webdriver.Chrome()  # You'll need to download chromedriver: https://sites.google.com/a/chromium.org/chromedriver/downloads\n",
    "\n",
    "    # Open the URL\n",
    "    driver.get(url)\n",
    "\n",
    "    while True:\n",
    "        # Parse the HTML content\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "        # Find the table containing the data\n",
    "        table = soup.find('table', class_='list_table')\n",
    "\n",
    "        # Check if the table exists\n",
    "        if table:\n",
    "            # Find all rows in the table\n",
    "            rows = table.find_all('tr')\n",
    "\n",
    "            # Loop through each row (skipping the header row)\n",
    "            for row in rows[1:]:\n",
    "                # Find all columns in the row\n",
    "                cols = row.find_all('td')\n",
    "\n",
    "                # Extract relevant data from each column\n",
    "                tender_id = cols[0].text.strip()\n",
    "                tender_description = cols[1].text.strip()\n",
    "                tender_organization = cols[2].text.strip()\n",
    "                tender_date = cols[3].text.strip()\n",
    "                other_column = cols[4].text.strip()  # Assuming there's one more column\n",
    "\n",
    "                # Append the data to the list\n",
    "                scraped_data.append({\n",
    "                    'Tender ID': tender_id,\n",
    "                    'Description': tender_description,\n",
    "                    'Organization': tender_organization,\n",
    "                    'Date': tender_date,\n",
    "                    'Other Column': other_column\n",
    "                })\n",
    "\n",
    "        # Find all \"Next\" buttons and click on the first one using XPath\n",
    "        next_buttons = driver.find_elements('xpath', '//a[@class=\"js\"]')\n",
    "        if next_buttons:\n",
    "            next_buttons[0].click()\n",
    "        else:\n",
    "            break  # Break the loop if there are no more pages\n",
    "\n",
    "    # Close the web driver\n",
    "    driver.quit()\n",
    "\n",
    "    return scraped_data\n",
    "\n",
    "# URL of the website\n",
    "url = \"https://eprocure.gov.in/cppp/resultoftendersnew/cpppdata/byYzJWc1pXTjBBMTNoMUExM2gxQTEzaDFBMTNoMU1qQXlNdz09QTEzaDFVSFZpYkdsemFHVms=\"\n",
    "\n",
    "# Scrape data from the URL\n",
    "data = scrape_data(url)\n",
    "\n",
    "# Print the number of entries scraped\n",
    "print(\"Total Entries Scraped:\", len(data))\n",
    "\n",
    "# Print the first few scraped entries\n",
    "for entry in data[:5]:\n",
    "    print(entry)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of times 'Next' button clicked: 1\n",
      "Total Entries Scraped: 10\n",
      "{'Tender ID': '1.', 'Description': '25-Feb-2024 12:00 AM', 'Organization': '31-Dec-2023 06:55 PM', 'Date': 'Providing and fixing of stainless steel gate and repairing of cow catcher at Mines Rescue Station Colony under Talcher Area./ MCL/GM(TA)/SO(C)/ e-Tender/23-24/49 dt 30.12.23 / 2023_MCL_297372_1', 'Other Column': 'Mahanadi Coalfields Limited||TALCHER AREA||TALCH_CIVIL||CIVIL'}\n",
      "{'Tender ID': '2.', 'Description': '26-Feb-2024 12:00 AM', 'Organization': '31-Dec-2023 06:55 PM', 'Date': 'Cleaning, sweeping along with minor repairs as and when required at Regional Store premises under Talcher Area for a period of one year./ MCL/GM(TA)/SO(C)/ e-Tender/23-24/50 dt 30.12.23 / 2023_MCL_297374_1', 'Other Column': 'Mahanadi Coalfields Limited||TALCHER AREA||TALCH_CIVIL||CIVIL'}\n",
      "{'Tender ID': '3.', 'Description': '08-Feb-2024 05:00 PM', 'Organization': '31-Dec-2023 05:00 PM', 'Date': '1)ALUMINIMUM SHIELDING TUBE L1=470, L=1170 AS PER VAR00 OF DRG. NO. 34528400012 REV01.\\n Qty: 1 NO/ E6933030/6935116 / 6935116', 'Other Column': 'BHEL BHOPAL'}\n",
      "{'Tender ID': '4.', 'Description': '31-Mar-2024 05:00 PM', 'Organization': '31-Dec-2023 05:00 PM', 'Date': '1)22W LED TUBE LIGHT FOR RETROFITTING AS PER SPEC BP7410399. Qty: 1500 NO/ E3833073/3830159 / 3830159', 'Other Column': 'BHEL BHOPAL'}\n",
      "{'Tender ID': '5.', 'Description': '06-Apr-2024 05:00 PM', 'Organization': '31-Dec-2023 05:00 PM', 'Date': '1)DIGITAL INFRARED NON-CONTACT THERMO METER WITH LASER,\\nMEASUREMENT TEMPERATURE RANGE: -50 TO 850 DEGREE\\nCELCIUS,\\nBASIC ACCURACY: +1.5 PERCENT  OF READING,\\nIR TEMPERATURE RESOLUTION: -0.1 C\\nEMISSIVITY: 0.1-1.0 (ABJUSTABLE),\\nTYPE OF DISPLA/ E9333057/9345001 / 9345001', 'Other Column': 'BHEL BHOPAL'}\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_data(url):\n",
    "    # Initialize an empty list to store the scraped data\n",
    "    scraped_data = []\n",
    "\n",
    "    # Initialize the web driver\n",
    "    driver = webdriver.Chrome()  # You'll need to download chromedriver: https://sites.google.com/a/chromium.org/chromedriver/downloads\n",
    "    \n",
    "    # Initialize a counter for the number of clicks on the \"Next\" button\n",
    "    next_button_clicks = 0\n",
    "\n",
    "    # Open the URL\n",
    "    driver.get(url)\n",
    "\n",
    "    while True:\n",
    "        # Increment the counter for the number of clicks on the \"Next\" button\n",
    "        next_button_clicks += 1\n",
    "\n",
    "        # Parse the HTML content\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "        # Find the table containing the data\n",
    "        table = soup.find('table', class_='list_table')\n",
    "\n",
    "        # Check if the table exists\n",
    "        if table:\n",
    "            # Find all rows in the table\n",
    "            rows = table.find_all('tr')\n",
    "\n",
    "            # Loop through each row (skipping the header row)\n",
    "            for row in rows[1:]:\n",
    "                # Find all columns in the row\n",
    "                cols = row.find_all('td')\n",
    "\n",
    "                # Extract relevant data from each column\n",
    "                tender_id = cols[0].text.strip()\n",
    "                tender_description = cols[1].text.strip()\n",
    "                tender_organization = cols[2].text.strip()\n",
    "                tender_date = cols[3].text.strip()\n",
    "                other_column = cols[4].text.strip()  # Assuming there's one more column\n",
    "\n",
    "                # Append the data to the list\n",
    "                scraped_data.append({\n",
    "                    'Tender ID': tender_id,\n",
    "                    'Description': tender_description,\n",
    "                    'Organization': tender_organization,\n",
    "                    'Date': tender_date,\n",
    "                    'Other Column': other_column\n",
    "                })\n",
    "\n",
    "        # Find all \"Next\" buttons and click on the first one using XPath\n",
    "        next_buttons = driver.find_elements('xpath', '//a[@class=\"js\"]')\n",
    "        if next_buttons:\n",
    "            next_buttons[0].click()\n",
    "        else:\n",
    "            break  # Break the loop if there are no more pages\n",
    "\n",
    "    # Close the web driver\n",
    "    driver.quit()\n",
    "\n",
    "    # Output the number of times the \"Next\" button has been clicked\n",
    "    print(\"Number of times 'Next' button clicked:\", next_button_clicks)\n",
    "\n",
    "    return scraped_data\n",
    "\n",
    "# URL of the website\n",
    "url = \"https://eprocure.gov.in/cppp/resultoftendersnew/cpppdata/byYzJWc1pXTjBBMTNoMUExM2gxQTEzaDFBMTNoMU1qQXlNdz09QTEzaDFVSFZpYkdsemFHVms=\"\n",
    "\n",
    "# Scrape data from the URL\n",
    "data = scrape_data(url)\n",
    "\n",
    "# Print the number of entries scraped\n",
    "print(\"Total Entries Scraped:\", len(data))\n",
    "\n",
    "# Print the first few scraped entries\n",
    "for entry in data[:5]:\n",
    "    print(entry)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-dde165dbd900>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[1;31m# Scrape data from the URL\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscrape_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;31m# Print the number of entries scraped\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-dde165dbd900>\u001b[0m in \u001b[0;36mscrape_data\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m     54\u001b[0m         \u001b[0mnext_buttons\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_elements\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'xpath'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'//a[@class=\"paginate_button\" and contains(text(), \"Next\")]'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnext_buttons\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m             \u001b[0mnext_buttons\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclick\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m             \u001b[1;32mbreak\u001b[0m  \u001b[1;31m# Break the loop if there are no more pages\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\webelement.py\u001b[0m in \u001b[0;36mclick\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     92\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclick\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;34m\"\"\"Clicks the element.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 94\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_execute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCommand\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCLICK_ELEMENT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0msubmit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\webelement.py\u001b[0m in \u001b[0;36m_execute\u001b[1;34m(self, command, params)\u001b[0m\n\u001b[0;32m    393\u001b[0m             \u001b[0mparams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    394\u001b[0m         \u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"id\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_id\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 395\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    396\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    397\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfind_element\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mby\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mBy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mID\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mWebElement\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py\u001b[0m in \u001b[0;36mexecute\u001b[1;34m(self, driver_command, params)\u001b[0m\n\u001b[0;32m    343\u001b[0m                 \u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"sessionId\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msession_id\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 345\u001b[1;33m         \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommand_executor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdriver_command\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    346\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    347\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_response\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\remote_connection.py\u001b[0m in \u001b[0;36mexecute\u001b[1;34m(self, command, params)\u001b[0m\n\u001b[0;32m    300\u001b[0m         \u001b[0mtrimmed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_trim_large_entries\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m         \u001b[0mLOGGER\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"%s %s %s\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcommand_info\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrimmed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 302\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_request\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand_info\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    304\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_request\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\remote_connection.py\u001b[0m in \u001b[0;36m_request\u001b[1;34m(self, method, url, body)\u001b[0m\n\u001b[0;32m    320\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    321\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeep_alive\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 322\u001b[1;33m             \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    323\u001b[0m             \u001b[0mstatuscode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    324\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\_request_methods.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, url, body, fields, headers, json, **urlopen_kw)\u001b[0m\n\u001b[0;32m    142\u001b[0m             )\n\u001b[0;32m    143\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 144\u001b[1;33m             return self.request_encode_body(\n\u001b[0m\u001b[0;32m    145\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfields\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfields\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0murlopen_kw\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m             )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\_request_methods.py\u001b[0m in \u001b[0;36mrequest_encode_body\u001b[1;34m(self, method, url, fields, headers, encode_multipart, multipart_boundary, **urlopen_kw)\u001b[0m\n\u001b[0;32m    277\u001b[0m         \u001b[0mextra_kw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murlopen_kw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    278\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 279\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mextra_kw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\poolmanager.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(self, method, url, redirect, **kw)\u001b[0m\n\u001b[0;32m    442\u001b[0m             \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    443\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 444\u001b[1;33m             \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mu\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest_uri\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    445\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    446\u001b[0m         \u001b[0mredirect_location\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mredirect\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_redirect_location\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    791\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    792\u001b[0m             \u001b[1;31m# Make the request on the HTTPConnection object\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 793\u001b[1;33m             response = self._make_request(\n\u001b[0m\u001b[0;32m    794\u001b[0m                 \u001b[0mconn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    795\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    535\u001b[0m         \u001b[1;31m# Receive the response from the server\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    536\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 537\u001b[1;33m             \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    538\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mBaseSSLError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mOSError\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    539\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_raise_timeout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout_value\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mread_timeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\connection.py\u001b[0m in \u001b[0;36mgetresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    464\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    465\u001b[0m         \u001b[1;31m# Get the response from http.client.HTTPConnection\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 466\u001b[1;33m         \u001b[0mhttplib_response\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    467\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    468\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1330\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1331\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1332\u001b[1;33m                 \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1333\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1334\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36mbegin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    301\u001b[0m         \u001b[1;31m# read until we get a non-100 response\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    302\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 303\u001b[1;33m             \u001b[0mversion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    304\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    305\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36m_read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    262\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    263\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 264\u001b[1;33m         \u001b[0mline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"iso-8859-1\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    265\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    266\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"status line\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    667\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    668\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 669\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    670\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    671\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_data(url):\n",
    "    # Initialize an empty list to store the scraped data\n",
    "    scraped_data = []\n",
    "\n",
    "    # Initialize the web driver\n",
    "    driver = webdriver.Chrome()  # You'll need to download chromedriver: https://sites.google.com/a/chromium.org/chromedriver/downloads\n",
    "    \n",
    "    # Initialize a counter for the number of clicks on the \"Next\" button\n",
    "    next_button_clicks = 0\n",
    "\n",
    "    # Open the URL\n",
    "    driver.get(url)\n",
    "\n",
    "    while True:\n",
    "        # Increment the counter for the number of clicks on the \"Next\" button\n",
    "        next_button_clicks += 1\n",
    "\n",
    "        # Parse the HTML content\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "        # Find the table containing the data\n",
    "        table = soup.find('table', class_='list_table')\n",
    "\n",
    "        # Check if the table exists\n",
    "        if table:\n",
    "            # Find all rows in the table\n",
    "            rows = table.find_all('tr')\n",
    "\n",
    "            # Loop through each row (skipping the header row)\n",
    "            for row in rows[1:]:\n",
    "                # Find all columns in the row\n",
    "                cols = row.find_all('td')\n",
    "\n",
    "                # Extract relevant data from each column\n",
    "                tender_id = cols[0].text.strip()\n",
    "                tender_description = cols[1].text.strip()\n",
    "                tender_organization = cols[2].text.strip()\n",
    "                tender_date = cols[3].text.strip()\n",
    "                other_column = cols[4].text.strip()  # Assuming there's one more column\n",
    "\n",
    "                # Append the data to the list\n",
    "                scraped_data.append({\n",
    "                    'Tender ID': tender_id,\n",
    "                    'Description': tender_description,\n",
    "                    'Organization': tender_organization,\n",
    "                    'Date': tender_date,\n",
    "                    'Other Column': other_column\n",
    "                })\n",
    "\n",
    "        # Find all \"Next\" buttons and click on the first one using XPath\n",
    "        next_buttons = driver.find_elements('xpath', '//a[@class=\"paginate_button\" and contains(text(), \"Next\")]')\n",
    "        if next_buttons:\n",
    "            next_buttons[0].click()\n",
    "        else:\n",
    "            break  # Break the loop if there are no more pages\n",
    "\n",
    "    # Close the web driver\n",
    "    driver.quit()\n",
    "\n",
    "    # Output the number of times the \"Next\" button has been clicked\n",
    "    print(\"Number of times 'Next' button clicked:\", next_button_clicks)\n",
    "\n",
    "    return scraped_data\n",
    "\n",
    "# URL of the website\n",
    "url = \"https://eprocure.gov.in/cppp/resultoftendersnew/cpppdata/byYzJWc1pXTjBBMTNoMUExM2gxQTEzaDFBMTNoMU1qQXlNdz09QTEzaDFVSFZpYkdsemFHVms=\"\n",
    "\n",
    "# Scrape data from the URL\n",
    "data = scrape_data(url)\n",
    "\n",
    "# Print the number of entries scraped\n",
    "print(\"Total Entries Scraped:\", len(data))\n",
    "\n",
    "# Print the first few scraped entries\n",
    "for entry in data[:5]:\n",
    "    print(entry)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of times 'Next' button clicked: 5\n",
      "Total Entries Scraped: 50\n",
      "{'Tender ID': '1.', 'Description': '26-Feb-2024 12:00 AM', 'Organization': '31-Dec-2023 06:55 PM', 'Date': 'Cleaning, sweeping along with minor repairs as and when required at Regional Store premises under Talcher Area for a period of one year./ MCL/GM(TA)/SO(C)/ e-Tender/23-24/50 dt 30.12.23 / 2023_MCL_297374_1', 'Other Column': 'Mahanadi Coalfields Limited||TALCHER AREA||TALCH_CIVIL||CIVIL'}\n",
      "{'Tender ID': '2.', 'Description': '25-Feb-2024 12:00 AM', 'Organization': '31-Dec-2023 06:55 PM', 'Date': 'Providing and fixing of stainless steel gate and repairing of cow catcher at Mines Rescue Station Colony under Talcher Area./ MCL/GM(TA)/SO(C)/ e-Tender/23-24/49 dt 30.12.23 / 2023_MCL_297372_1', 'Other Column': 'Mahanadi Coalfields Limited||TALCHER AREA||TALCH_CIVIL||CIVIL'}\n",
      "{'Tender ID': '3.', 'Description': '08-Feb-2024 05:00 PM', 'Organization': '31-Dec-2023 05:00 PM', 'Date': '1)ALUMINIMUM SHIELDING TUBE L1=470, L=1170 AS PER VAR00 OF DRG. NO. 34528400012 REV01.\\n Qty: 1 NO/ E6933030/6935116 / 6935116', 'Other Column': 'BHEL BHOPAL'}\n",
      "{'Tender ID': '4.', 'Description': '29-Feb-2024 05:00 PM', 'Organization': '31-Dec-2023 05:00 PM', 'Date': '1)\\nBAR CT RATIO:800-400/1-1A, CORE-1: 10VA,5P10 ,CORE-II: 10VA,0.5,AS PER ANNEX.-A IT-02. Qty: 18 NO/ E4933097/4935222 / 4935222', 'Other Column': 'BHEL BHOPAL'}\n",
      "{'Tender ID': '5.', 'Description': '02-Feb-2024 05:00 PM', 'Organization': '31-Dec-2023 05:00 PM', 'Date': '1)SPARE OF COOLER CONTROL CABINET TO MID-406735006 Qty: 1 ST/ E6733023/6735034 / 6735034', 'Other Column': 'BHEL BHOPAL'}\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_data(url):\n",
    "    # Initialize an empty list to store the scraped data\n",
    "    scraped_data = []\n",
    "\n",
    "    # Initialize the web driver\n",
    "    driver = webdriver.Chrome()  # You'll need to download chromedriver: https://sites.google.com/a/chromium.org/chromedriver/downloads\n",
    "    \n",
    "    # Initialize a counter for the number of clicks on the \"Next\" button\n",
    "    next_button_clicks = 0\n",
    "\n",
    "    # Open the URL\n",
    "    driver.get(url)\n",
    "\n",
    "    for _ in range(5):  # Iterate only 5 times for the first 5 pages\n",
    "        # Increment the counter for the number of clicks on the \"Next\" button\n",
    "        next_button_clicks += 1\n",
    "\n",
    "        # Parse the HTML content\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "        # Find the table containing the data\n",
    "        table = soup.find('table', class_='list_table')\n",
    "\n",
    "        # Check if the table exists\n",
    "        if table:\n",
    "            # Find all rows in the table\n",
    "            rows = table.find_all('tr')\n",
    "\n",
    "            # Loop through each row (skipping the header row)\n",
    "            for row in rows[1:]:\n",
    "                # Find all columns in the row\n",
    "                cols = row.find_all('td')\n",
    "\n",
    "                # Extract relevant data from each column\n",
    "                tender_id = cols[0].text.strip()\n",
    "                tender_description = cols[1].text.strip()\n",
    "                tender_organization = cols[2].text.strip()\n",
    "                tender_date = cols[3].text.strip()\n",
    "                other_column = cols[4].text.strip()  # Assuming there's one more column\n",
    "\n",
    "                # Append the data to the list\n",
    "                scraped_data.append({\n",
    "                    'Tender ID': tender_id,\n",
    "                    'Description': tender_description,\n",
    "                    'Organization': tender_organization,\n",
    "                    'Date': tender_date,\n",
    "                    'Other Column': other_column\n",
    "                })\n",
    "\n",
    "        # Find all \"Next\" buttons by class name and href attribute\n",
    "        next_buttons = driver.find_elements('xpath', '//a[@class=\"paginate_button\" and contains(text(), \"Next\")]')\n",
    "        if next_buttons:\n",
    "            # Click on the first \"Next\" button\n",
    "            next_buttons[0].click()\n",
    "        else:\n",
    "            break  # Break the loop if there are no more pages\n",
    "\n",
    "    # Close the web driver\n",
    "    driver.quit()\n",
    "\n",
    "    # Output the number of times the \"Next\" button has been clicked\n",
    "    print(\"Number of times 'Next' button clicked:\", next_button_clicks)\n",
    "\n",
    "    return scraped_data\n",
    "\n",
    "# URL of the website\n",
    "url = \"https://eprocure.gov.in/cppp/resultoftendersnew/cpppdata/byYzJWc1pXTjBBMTNoMUExM2gxQTEzaDFBMTNoMU1qQXlNdz09QTEzaDFVSFZpYkdsemFHVms=\"\n",
    "\n",
    "# Scrape data from the URL\n",
    "data = scrape_data(url)\n",
    "\n",
    "# Print the number of entries scraped\n",
    "print(\"Total Entries Scraped:\", len(data))\n",
    "\n",
    "# Print the first few scraped entries\n",
    "for entry in data[:5]:\n",
    "    print(entry)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(r\"C:\\Users\\dgarg\\Downloads\\data extracted\\gov.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
